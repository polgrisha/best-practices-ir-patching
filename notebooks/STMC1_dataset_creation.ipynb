{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import nltk\n",
    "import fasttext\n",
    "\n",
    "client = OpenAI(api_key = open('/mnt/qb/work/eickhoff/esx208/openai_keys/health_nlp_key.txt').read())\n",
    "ft_model = fasttext.load_model('/mnt/qb/work/eickhoff/esx208/data/fasttext_models/fasttext-en-vectors/model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_semantically_similar_terms_batches(query_term_table, openai_client, batch_size=5, word_num=3):\n",
    "    prompt = '''\n",
    "You will be given a user query and a single word from it. For a given word you should generate {word_num} semantically similar words given the query as a context.\n",
    "The generated words must not exist in the query. Please sort them from the most similar to the least.\n",
    "Your output should only be a list of dicts as in the example. It should be readable with the json.loads() function. \n",
    "Please generate similar words only to the word in the field \"word\".\n",
    "Example input: \n",
    "[\n",
    "    {{\n",
    "        \"query\": \"define preventive\",\n",
    "        \"word\": \"preventive\"\n",
    "    }},\n",
    "    {{\n",
    "        \"query\": \"here there be dragons comic\",\n",
    "        \"word\": \"comic\"\n",
    "    }},\n",
    "]\n",
    "Example output:\n",
    "[\n",
    "    {{\n",
    "        \"query\": \"define preventive\",\n",
    "        \"word\": \"preventive\",\n",
    "        \"synonyms\": [\"prophylactic\", \"preemptive\", \"averting\"]\n",
    "    }},\n",
    "    {{\n",
    "        \"query\": \"here there be dragons comic\",\n",
    "        \"word\": \"comic\",\n",
    "        \"synonyms\": [\"manga\", \"graphic\", \"cartoon\"]\n",
    "    }},\n",
    "]\n",
    "Input:\n",
    "{input}\n",
    "Output:\n",
    "'''\n",
    "    output = []\n",
    "    curr_input = []\n",
    "    for idx, line in tqdm(query_term_table.iterrows(), total=query_term_table.shape[0]):\n",
    "        curr_input.append({\n",
    "            \"query\": line['query'],\n",
    "            \"term\": line['inj_term']\n",
    "        })\n",
    "        if len(curr_input) == batch_size or idx == query_term_table.shape[0] - 1:\n",
    "            completion = openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt.format(input=json.dumps(curr_input, indent=4), word_num=word_num)\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            try:\n",
    "                output += json.loads(completion.choices[0].message.content)\n",
    "            except:\n",
    "                pass\n",
    "            curr_input = []\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load TFC1 data and try to replace appended words with their syunonyms using GPT4-o. We will then save the resulting dataset as STMC1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1056433/1644010316.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tfc1_data_perturbed['inj_term'] = tfc1_data_perturbed['text'].apply(lambda text: text.split(' ')[-1])\n"
     ]
    }
   ],
   "source": [
    "tfc1_data = pd.read_csv('/mnt/qb/work/eickhoff/esx208/MechIR/data/TFC1-data.tsv.gz', sep='\\t')\n",
    "tfc1_data_perturbed = tfc1_data[tfc1_data['perturbed'] == True]\n",
    "tfc1_data_perturbed['inj_term'] = tfc1_data_perturbed['text'].apply(lambda text: text.split(' ')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a102d47fc80d4c9f88c0241998b5df7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "semantically_similar_terms_batches = find_semantically_similar_terms_batches(tfc1_data_perturbed[['query', 'inj_term']].drop_duplicates(), client, batch_size=5, word_num=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_semantically_similar_pair_fasttext(semantically_similar_terms, ft_model):\n",
    "    semantically_similar_pairs = dict()\n",
    "    \n",
    "    for item in tqdm(semantically_similar_terms):\n",
    "        tokenized = nltk.word_tokenize(item['query'])\n",
    "        tokenized = [term.lower() for term in tokenized if term.isalpha()]\n",
    "        similar_words = [term.lower() for term in item['synonyms'] if not term.lower() in tokenized and not term.lower() in item['query']]\n",
    "        \n",
    "        if len(tokenized) > 0 and len(similar_words) > 0:\n",
    "            query_vectors = np.stack([ft_model.get_word_vector(term) for term in tokenized])\n",
    "            similar_words_vectors = np.stack([ft_model.get_word_vector(term) for term in similar_words])\n",
    "\n",
    "            query_vectors /= np.linalg.norm(query_vectors, axis=-1)[:, np.newaxis]\n",
    "            similar_words_vectors /= np.linalg.norm(similar_words_vectors, axis=-1)[:, np.newaxis]\n",
    "\n",
    "            scores = query_vectors @ similar_words_vectors.T\n",
    "            argmax = np.unravel_index(scores.argmax(), scores.shape)\n",
    "            semantically_similar_pairs[(item['query'], \n",
    "                                        item['term'] if 'term' in item else item['word'])] = (tokenized[argmax[0]], similar_words[argmax[1]])\n",
    "    return semantically_similar_pairs\n",
    "\n",
    "def select_semantically_similar_pair_first_possible(semantically_similar_terms):\n",
    "    semantically_similar_pairs = dict()\n",
    "    for item in semantically_similar_terms:\n",
    "        tokenized = nltk.word_tokenize(item['query'])\n",
    "        tokenized = [term.lower() for term in tokenized if term.isalpha()]\n",
    "        for word in item['synonyms']:\n",
    "            if not word.lower() in tokenized:\n",
    "                if 'word' in item:\n",
    "                    semantically_similar_pairs[item['query']] = (item['word'], word)\n",
    "                else:\n",
    "                    semantically_similar_pairs[item['query']] = (item['term'], word)\n",
    "    return semantically_similar_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8bc15e0c39440f0a1c9c0c196c8e761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "semantically_similar_pairs = select_best_semantically_similar_pair_fasttext(semantically_similar_terms_batches, ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfc1_data_perturbed_new = tfc1_data_perturbed.copy()\n",
    "tfc1_data_perturbed_new = tfc1_data_perturbed_new[tfc1_data_perturbed_new.apply(lambda row: (row['query'], row['inj_term']) in semantically_similar_pairs, axis=1)]\n",
    "tfc1_data_perturbed_new['synonym'] = tfc1_data_perturbed_new.apply(lambda row: semantically_similar_pairs[(row['query'], row['inj_term'])][1], axis=1)\n",
    "tfc1_data_perturbed_new['inj_term'] = tfc1_data_perturbed_new.apply(lambda row: semantically_similar_pairs[(row['query'], row['inj_term'])][0], axis=1)\n",
    "tfc1_data_perturbed_new['text'] = tfc1_data_perturbed_new.apply(lambda row: row['text'].rsplit(' ', 1)[0] + ' ' + row['synonym'], axis=1)\n",
    "\n",
    "tfc1_data_non_perturbed = tfc1_data[tfc1_data['perturbed'] == False].copy()\n",
    "tfc1_data_non_perturbed = tfc1_data_non_perturbed.merge(tfc1_data_perturbed[['qid', 'docno', 'inj_term']], on=['qid', 'docno'], how='inner')\n",
    "tfc1_data_non_perturbed = tfc1_data_non_perturbed[tfc1_data_non_perturbed.apply(lambda row: (row['query'], row['inj_term']) in semantically_similar_pairs, axis=1)]\n",
    "tfc1_data_non_perturbed['inj_term'] = None\n",
    "\n",
    "stmc1_data = pd.concat([tfc1_data_non_perturbed, tfc1_data_perturbed_new], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stmc1_data.to_csv('/mnt/qb/work/eickhoff/esx208/MechIR/data/STMC1-data.tsv.gz', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
